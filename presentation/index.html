<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Timeline Presentation</title>
  <link rel="stylesheet" href="./styles.css">
</head>
<body>
  <div class="timeline">
    <div class="timeline-navigation">
      <div class="timeline-node active" data-target="event-1">INTRO</div>
      <div class="timeline-node" data-target="event-2">DATA</div>
      <div class="timeline-node" data-target="event-3">ANNOTATION</div>
      <div class="timeline-node" data-target="event-4">TESTING</div>
      <div class="timeline-node" data-target="event-5">PIPELINE</div>
      <div class="timeline-node" data-target="event-6">EVALUATION</div>
      <div class="timeline-node" data-target="event-7">DEPLOY</div>
      <div class="demo-node" data-target="demo">DEMO</div>
    </div>
    <div class="event-container">
      <div class="event" id="event-1"> 
        <div class="event-section">
          <h3 class ="event-title">The Usefulness of Social Media for Understanding Adverse Drug Reactions</h3>
          <p>The rapid expansion and ubiquity of social media platforms have generated vast amounts of user-generated content, offering unprecedented opportunities for the exploration of various research questions. In the realm of pharmacovigilance, investigating the degree to which adverse medication reactions are shared on social media beyond official reporting channels is of particular interest. This study aims to identify potential unacknowledged adverse drug reactions (ADRs) that may be absent from the FDA's Adverse Event Reporting System (FAERS) and determine whether significant outliers exist, while ascertaining the reliability of social media as a data source for pharmacovigilance.</p>
        </div>
        
        <div class="event-section">
          <p>To accomplish this, we will conduct a comparative analysis of the reporting rates of ADRs for the most prescribed medications in the United States, comparing data extracted from social media platforms with that from FAERS. Our measurable objectives include collecting ADR data from both sources, developing ADR extraction methods for social media, comparing social media ADR rates to FAERS baselines, and identifying unrecognized ADRs and significant outliers. Due to constraints, our analysis will be limited to a specific number of medications and will consider Reddit-specific limitations, such as the platform's younger demographic and longer comments. Additionally, we will account for differences in data collection methods and potential social media biases.</p>
        </div>
        
        <div class="event-section">
          <p>Our initial methodology encompasses research methods such as Reddit API utilization and FAERS data mining among a multitude of modelling methods. The intended audience for our findings consists of pharmaceutical, bioinformatics, and public health researchers. We will present our results through a comprehensive website that features visualizations, dashboards, and statistics, offering a detailed exploration of social media data mining for pharmacovigilance.</p>
        </div>
      </div>    
        
      <div class="event" id="event-2"> 
        <div class="event-section">
          <h3 class ="event-title">Data Sources</h3>
          <p>The initial data sources for this study are the FDA's Adverse Event Reporting System (FAERS) and Reddit, as they provide rich and diverse perspectives on adverse drug reactions (ADRs). FAERS is the standard source of information on ADRs reported to the FDA, with voluntary reporting from healthcare providers, patients, and pharmaceutical companies. Reddit, on the other hand, serves as a platform for communities to discuss various diseases and their treatments, offering detailed reports of side effects and potentially unreported ADRs. Utilizing the respective APIs for each data source allows for targeted searches of specific medications.</p>
        </div>
        
        <div class="event-section">
          <p>To efficiently extract Reddit data, we employed the Python Reddit API Wrapper (PRAW) and PushshiftAPI for targeted searches of comments mentioning specific medications. Challenges in Reddit data extraction include the length and grammar of comments, as well as the lack of context, which may hinder accurate interpretation and understanding of the full scope of ADRs. To address these challenges and ensure secure, scalable data storage and processing, we implemented a data management strategy using Jupyter and Google Cloud.</p>
        </div>
        
        <div class="event-section">
          <h3 class ="event-title">Exploratory Data Analysis</h3>
          <p>Sentiment analysis was conducted using the SentimentIntensityAnalyzer on Reddit comment content to determine the polarity scores and assign sentiment labels to DataFrame rows. Visualization of the results was achieved through Seaborn box plots and count plots. Basic topic modeling of negative Reddit comments was performed using Latent Dirichlet Allocation (LDA). Sentiment analysis is a valuable tool for identifying the underlying tone and sentiment of user-generated content, enabling us to uncover potential ADRs that may not be evident through traditional data mining techniques.</p>
          <div class="image-container">
            <img src="visualizations/sent_analysis.png" alt="Sentiment Analysis">
            <img src="visualizations/sent_analysis_dist.png" alt="Sentiment Analysis Distribution">
          </div>
        
        </div>
        
        <div class="event-section">
          <p>To further explore the data, we used the wordcloud library to visualize the most frequently used words per medication and colored these words based on sentiment analysis conducted with the TextBlob library. Moreover, we employed pandas to compare word frequencies across different sentiments, calculating frequency differences for positive and negative Reddit comments. These visualizations and comparisons provide insights into the prominence and distribution of sentiments related to specific medications, allowing for the identification of ADR trends and significant outliers within the data.</p>
          <div class="image-container">
            <img src="visualizations/wordclouds.png" alt="Wordcloud">
            <img src="visualizations/wordfreqcomp.png" alt="Word Frequency Comparison">
          </div>
        </div>
      </div>

	    <div class="event" id="event-3">
        <div class="event-section">
          <h3 class ="event-title"><b>Pivot to Deep Learning</b></h3>
            <p>Recognizing the complexity of our problem, we decided to explore neural networks and deep learning methods under the guidance of our mentor. This required a transformation in our approach, which involved narrowing our scope from multiple medications to a single medication for building a robust model. Subsequently, we aimed to develop a pipeline that could be applied to other drugs.</p>
        </div>
        <div class="event-section">  
          <h3 class ="event-title"><b>spaCy NLP Library Compatibility w/ Prodigy</b></h3>
            <p>The spaCy library is a powerful and widely-used natural language processing (NLP) library designed for efficient and high-performance text processing tasks. It offers pre-trained models that are developed on large and diverse datasets, ensuring versatility and applicability across various domains. One of the key strengths of spaCy is its fine-tunability, which enables us to achieve higher accuracy and better performance by adapting the pre-trained models to specific tasks or domains.</p>         
            <p>Prodigy, an annotation tool developed by the creators of spaCy, is designed to streamline and optimize the annotation process. Prodigy offers the ability to suggest new annotations based on patterns defined by the user, significantly reducing the time and effort required for manual annotation. Furthermore, the tool supports multiple users, allowing us to asynchronously annotate documents and collaborate more effectively.</p>
            <p>Another advantage of Prodigy is its seamless integration with Google Cloud Platform (GCP), which enables us to scale our model training and deployment to handle large volumes of data. This integration ensures that our NLP pipeline can be easily adapted to meet the demands of various projects and workloads.</p>
          </div>
        <div class="event-section">  
          <h4 class ="event-title"><b>Inter-rater Reliability</b></h4>      
            <p>
              We redefined our key performance indicators to include Cohen's Kappa calculations, ensuring inter-rater reliability (IRR) between annotator pairs. IRR is essential for maintaining consistency and accuracy in annotation tasks, minimizing subjective bias and variability among annotators, and ultimately enhancing the quality of training data and overall model performance. Cohen's Kappa, a statistical measure that quantifies the level of agreement between two annotators while accounting for chance agreement, served as a valuable metric. Kappa values range from -1 to 1, with 1 indicating perfect agreement and values above 0.6 generally considered acceptable.</p>
        </div>
        <div class="event-table">
          <div class="header">
              <h3 class="event-title"><b>Prodigy Annotation Styles</b></h3>
          </div>
          <div class="content">
              <p>
                  <h4><a href="https://demo.prodi.gy/?=null&view_id=ner_manual">Named Entity Recognition (NER)</a></h4>
                  <ul>
                      <li>Proper nouns and self-contained expressions like person names or products</li>
                      <li>Single-token-based tags; better with clear token boundaries</li>
                  </ul>
              </p>
          </div>
          <div class="content">
              <p>
                <h4><a href="https://demo.prodi.gy/?=null&view_id=spans_manual">Span Categorization (SpanCat)</a></h4>
                  <ul>
                      <li>Multi-token-based tags; better with complex or overlapping entities</li>
                      <li>Can be used to annotate relations between entities</li>
                  </ul>
              </p>
          </div>
          <div class="content">
              <p>
                  <h4><a href="https://demo.prodi.gy/?=null&view_id=textcat">Text Classification (TextCat)</a></h4>
                  <ul>
                      <li>Binary or multi-class classification of text</li>
                      <li>Can be used to annotate text with a single or multiple labels</li>
                  </ul>
              </p>
          </div>
        
          <div class="image">
              <img src="visualizations/ner.png" width=550 alt="NER Example" class="padding">
          </div>
          <div class="image">
              <img src="visualizations/spancat.png" width=550 alt="SpanCat Example" class="padding">
          </div>
          <div class="image">
              <img src="visualizations/textcat.png" width=550 alt="TextCat Example" class="padding">
          </div>
        </div>
        <div class="event-section">
          <h3 class ="event-title"><b>Choosing a Medication</b></h3>
            <p>We chose Ocrevus (ocrelizumab), due to it being a recently approved multiple sclerosis (MS) medication considered "first-in-class" by the FDA.</p>
            <p>At this point we also chose the Medical Dictionary for Regulatory Activities (MedDRA) terminology of side effects as a resource to use as patterns for Prodigy to enhance our annotation process.</p>
        </div>
        <div>
          <h4>Initial annotation attempts</h4>
          <p>
            NER:
            <ul>
              <li>Failed attempt due to unclear annotation rules, switched to Span Categorization as we felt NER may have contributed to the issue at the time.</li>
            </ul>
            SpanCat:
            <ul>
              <li>Zach and Taylor's Cohen's Kappa: <b>.46</b></li>
              <li>Aidan and Jackson's Cohen's Kappa: <b>.34</b></li>
            </ul>
          </p>
        </div>
      </div>

	    <div class="event" id="event-4">
        <div class="event-section">
          <h3 class ="event-title"><b>Annotation Updates and Model Testing</b></h3>
            <p>
              During our testing phase, we realized that Ocrevus, despite being popular on Reddit, was not an ideal medication for our study due to its relatively low number of adverse drug reactions (ADRs). To provide the model with sufficient ADR data for training, we switched to studying Humira—a tumor necrosis factor (TNF) blocker known for reducing inflammation—observing that many individuals mentioned and complained about it while annotating Ocrevus comments.
            </p>
        </div>
        <div>
          <h4 class ="event-title">Annotation Updates</h4>
          <p>
            To expedite the annotation process, we updated our patterns file by incorporating resources such as the ADR Lexicon V 1.1 from HLP Cedars-Sinai, SIDER, CHV, COSTART, and DIEGO_Lab for ADRs, and Drugs@FDA Data Files for drug names and active ingredients. Integrating these resources into our annotation process enabled us to increase annotation speed and improve the quality of our training data, thereby enhancing the overall performance of our model.
          </p>
          <h4>IRR improvements</h4>

          <p>
            SpanCat (from previous SpanCat):

            <ul>
              <li>Taylor and Zach: .46 -> <b>.55</b></li>
              <li>Aidan and Jackson : .34 -> <b>.56</b></li>
            </ul>
            

            TextCat:
            
            <ul>
              <li>Taylor and Zach: <b>.74</b></li>
              <li>Aidan and Jackson: <b>.58</b></li>
            </ul>

          </p>
        </div>
        <div class="event-section">
          <h4 class ="event-title"><b>Sense2Vec</b></h4>
            <p>
              Another tool from the creators of spaCy and Prodigy, that had the potential to assist in our labeling of drugs and ADRs was Sense2Vec. Sense2Vec is a novel method for word sense disambiguation in neural word embeddings that utilizes supervised NLP labels instead of unsupervised clustering. Sense2Vec can disambiguate different parts of speech, sentiment, named entities, and syntactic roles of words, as well as demonstrate subjective and qualitative examples of disambiguated embeddings. We were able to add and utilize the new side effect vectors (which were the same as our new patterns) and pick single word ADRs out with regularity but struggled with multi-word phrases. We also attempted drug name vectors, but beyond very common drugs the results were less than satisfactory.            
            </p>
        </div>
        <div class="event-section">
          <h4 class ="event-title"><b>Remedying Class Imbalances with OpenAI's gpt-3.5-turbo API</b></h4>
            <ul>
              <li>Useful in creating examples that don't naturally occur very often in the data.</li>
              <li>Able to generate similar perspectives to examples provided that may not be present in the original dataset.</li>
              <li>Potential to counter biased data and recognize anomalies.</li>
              <li>Reduce manual annotation workload through the usage of effective prompt engineering.</li>
            </ul>
        </div>
        <div class="event-section">
          <h4 class ="event-title"><b>GPT-2</b></h4>
            <p>
              We also tested a GPT-2 model with a classifier for ADR identification. We found that it was "learning" the general format of the ChatGPT comments and started to label any shorter comment an ADR. 
              
              Some were correctly classified, but overall, the model results were underwhelming.
            </p>
            <img src="visualizations/gpt2-train.png" alt="GPT-2 Training" class="padding">
            <p>Our final pipeline's structure begins to emerge during this testing phase, but we knew we needed to revamp and improve our models as well as find a way to not only label the drugs and ADRs but also link them together.</p>
        </div>     
      </div> 

	    <div class="event" id="event-5">
        <div class="event-section">
          <h3 class ="event-title"><b>Pipeline Development</b></h3>
            <p>
              Our primary goal was to develop a reproducible pipeline that enables the analysis of not only Reddit comments but also extends to other social media platforms, such as Twitter and Facebook. This pipeline aims to be accessible and usable by academic, corporate, governmental, and public entities, allowing users to input any medication or adverse drug reaction of interest.            
            </p>
            <h4>Stage 1: RoBERTa Text Classification</h4>
            <div class="stage-container">
              <div class="stage-text">
                <p>
                  We employed the RoBERTa text classification model, a robustly optimized BERT pretraining approach built on the classic BERT model with longer and more focused pretraining and hyperparameter optimization. Utilizing the pre-trained RoBERTa base model from the HuggingFace transformers library, we developed custom PyTorch datasets and dataloaders for text preprocessing, which involved removing punctuation and links.
                </p> 
                <p>
                  The comments were split into lists of strings and passed into the RoBERTa tokenizer item by item with overlap. The PyTorch classification head on top of the RoBERTa base model takes the pooled output and performs classification (ADR or no ADR) with dropout layers to prevent overfitting. We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler.            
                </p>
                <p>
                  To augment text classification training data, we employed OpenAI's GPT-3.5-turbo model for generating and classifying comments, addressing class imbalances and diversifying the training data to counteract biases and improve annotation efficiency.            
                </p>
              </div>
              <div class="stage-image">
                <img src="visualizations/flair f1 score improvement.png" alt="Stage 1 Image">
              </div>
            </div>
            <h4>Stage 2: Flair NER</h4>
            <div class="stage-container">
              <div class="stage-text">
                <p>
                  For Named Entity Recognition (NER), we used Flair's SequenceTagger with stacked embeddings, including GloVe and Flair embeddings, providing different embeddings for the same word depending on its contextual use. 
                </p>
                <p>
                  We incorporated word dropout and locked dropout to prevent overfitting, as well as a bidirectional Long Short-Term Memory (biLSTM) RNN to maintain short-term memory throughout the input sequence processing. 
                </p>
                <p>
                  We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler, employing the ViterbiLoss function.            
                </p>
              </div>
              <div class="stage-image">
                <img src="visualizations/roberta f1 score improvement.png" alt="Stage 1 Image">
              </div>
            </div>
            
            
            
            <h4>Stage 3: spaCy Dependency Parser</h4>
            <p>
              Finally, we utilized a <b>pre-trained</b> spaCy dependency parser to extract and pair drug-ADR entities based on the output of the SequenceTagger. This parser links drugs and ADRs in the input text by finding the shortest dependency path between them, using the en_core_web_md model. The extracted and paired drug-ADR entities were saved in a CSV file for further analysis.            </p>
            <h4>Pipeline Summary</h4>
            <p>
              This pipeline offers a robust and flexible approach to extracting ADR phrases in relation to medications in text, making it an optimal method for our project's objectives. By combining advanced text classification, NER, and dependency parsing techniques, our pipeline can efficiently identify and extract relevant information from social media data for pharmacovigilance purposes.            </p>
        </div> 
      </div>

      <div class="event" id="event-6">
        <div class="event-section">
          <h3 class ="event-title"><b>Pipeline Evaluation and Optimization</b></h3>
            <p>
              During the final annotation phase, Taylor and Zach maintained a satisfactory Cohen's Kappa for text classification, while Jackson and Aidan initially achieved poor results. To address this issue, the team created a document outlining rules for both annotation methods. Subsequently, Jackson and Aidan significantly improved their Cohen's Kappa scores, achieving better consistency and accuracy in their annotations. The inter-rater reliability improvements were as follows:

              <ul><li>Text Classification (from previous TextCat):</li>
                    <ul>
                      <li>Taylor and Zach: 0.74 -> <b>0.73</b></li>
                    </ul>
                  <li>Named Entity Recognition:</li>
                  <ul>
                    <li>Aidan and Jackson: 0.38* -> <b>0.58</b></li>
                  </ul>
              </ul>            
            </p>
        </div>
        <div class="event-section">
          <h3 class ="event-title"><b>RoBERTa Text Classification</b></h3>
            <p>
              To evaluate the RoBERTa model, we split the data into training, validation, and test sets. The model was trained using a custom PyTorch data loader and the RoBERTa tokenizer, which is a byte pair encoding (BPE) tokenizer. We ran the model for five epochs with a validation cycle after each epoch and calculated the training and validation accuracy and loss metrics at each step to observe convergence.            
            </p>
            <img src="visualizations/robertatrainviz.png" alt="RoBERTa Training" class="padding">
            <p>
              To augment text classification training data, we employed OpenAI's GPT-3.5-turbo model for generating and classifying comments, addressing class imbalances and diversifying the training data to counteract biases and improve annotation efficiency.            
            </p>
        </div>
        <div class="event-section">
          <h3 class ="event-title"><b>Flair NER</b></h3>
            <p>
              For Named Entity Recognition (NER), we used Flair's SequenceTagger with stacked embeddings, including GloVe and Flair embeddings, providing different embeddings for the same word depending on its contextual use. We incorporated word dropout and locked dropout to prevent overfitting, as well as a bidirectional Long Short-Term Memory (biLSTM) RNN to maintain short-term memory throughout the input sequence processing. We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler, employing the ViterbiLoss function.            
            </p>
            <h4>Flair Training Metrics (Relaxed Annotations)</h4>
            <p>In these classification reports (left is with only GloVe embeddings; right is GloVe embeddings AND Flair embeddings), we see pretty stellar results with accuracies of <b>0.73</b> and <b>0.78</b> respectively. However, we needed a better annotation strategy between the two raters and better training metrics other than a classification report to understand whether our model was fitting correctly.</p>
            <div class="image-container">
              <img src="visualizations/flairjustglove.png" alt="NER Visualization">
              <img src="visualizations/flairembedplusglove.png" alt="NER Visualization">
            </div>
            <h4>Flair Training Metrics (Strict Annotations)</h4>
            <p>With regards to the changes needed for our annotations, one of the major questions we needed to ask ourselves was whether we should annotate all ADR-like words as ADR when they occur or only when they occur in an sentence using the word in the context of an ADR? To test this, we decided to do a round of annotations where we used the latter method, which we called the <em>strict</em> method, only marking a word as an ADR if it was mentioned in a sentence as an ADR.</p>
            <img src="visualizations/strictflairtrain.png" alt="Flair Training" class="padding">
            <p>The problem with the strict labeling style is that by being so selective with labeling ADRs in the text we were unable to provide enough examples of ADRs in the training data for the model to train on as a majority of the words that could be ADRs but were marked as not teaches the model not to label those words at all. In turn, the model was underfit, unable to learn from the training dataset at all. Along with this, the accuracy, precision, recall, and f1-score all plateaud showing no improvement.</p>
            <h4>Flair Training Metrics (Relaxed + Strict Annotations)</h4>
            <p>To resolve some of these issues, we decided to combine the two annotated datasets, 1/2 strict 1/2 relaxed, hoping to provide a variety of examples in the training data to the model.</p>
            <img src="visualizations/flairtrainviz.png" alt="Flair Training" class="padding">
            <p>As you can see, our model was able to fit to the training data and showed improvement in all other metrics. Leading us to be confident in it's ability to label our data effectively.</p>
        </div>
        <div class="event-section">
          <h3 class ="event-title"><b>spaCy Dependency Parser</b></h3>   
            <p>
              For our dependency parser implementation, we have developed an interactive network graph that visually represents the relationships between drug and ADR entities extracted using the NER model. This graph, constructed by generating nodes for each unique drug and ADR entity and forming edges based on the dependency relations identified by the spaCy dependency parser, allows users to explore the connections between drugs and their associated ADRs, thereby providing an intuitive way to assess the quality of the entity extraction and dependency parsing processes. The interactive visualization enables users to click on individual nodes and view the corresponding text containing the extracted pairing, offering valuable insights into the context in which the entities were identified and the accuracy of the dependency parsing process.
            </p>
            <img src="visualizations/dependency-viz.png" width="550" alt="Dependency Parser Visualization" class="padding">
            <p>
              This evaluation method offers several advantages for understanding the performance of the NER and dependency parsing models, such as facilitating a more intuitive understanding of the underlying structure of the extracted entities and their associations, and allowing users to explore specific instances of drug-ADR pairings to assess their validity based on the surrounding context. Furthermore, the integration of NER results and dependency parser output enables users to evaluate the overall effectiveness of the pipeline in identifying and connecting relevant entities.            
            </p>
        </div> 
      </div>

      <div class="event" id="event-7">
        <div class="event-section"> 
          <h3 class ="event-title"><b>Docker Containers for Data Storage and Reproducibility</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage with two initial versions (v1: 1200 comments, v1-large: 12000 comments)</li>
            <li>Ensures reproducibility through containerized environments</li>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>GitHub Repository and Actions Workflows</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage and version control</li>
            <li>GitHub-hosted CPU runners and self-hosted GPU runners</li>
            <li>Initial workflows:</li>
            <ul>
                <li>Push select GitHub branch to a specific Docker tag</li>
                <li>Enter medication, select Docker tag, run pipeline, and deploy to Shiny Apps (on self-hosted runner)</li>
            </ul>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>Challenges and Workflow Adjustments</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Aidan's laptop's RTX 2060 Max-Q can only do so much...</li>
            <li>Running pipeline for each user request is time-consuming and impractical</li>
            <li>New strategy considered for more efficient processing</li>
            <li>PushShift API issues encountered</li>
            <li>Config file refresh leads to hiccups when running, usually needs to be reset once daily, but works otherwise.</li>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>Nautilus HyperCluster</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage, CPU and GPU runners</li>
            <li>Initial workflows:</li>
            <ul>
                <li>Run pipeline and deploy to Shiny Apps</li>
            </ul>
            <li>Current Workflow:</li>
            <ul>
                <li>Create a database of Reddit comments using Medicrawl.sh</li>
                <ul>
                    <li>Pulls 1000 comments for each medication listed in ConfigMap file</li>
                    <li>Appends comments to CSVs stored in Persistent Volume Claim (PVC)</li>
                </ul>
                <li>GitHub and Nautilus integration:</li>
                <ul>
                    <li>Push to Shiny App: loads CSVs from PVC and builds and deploys Shiny App</li>
                    <li>Push to Pages: creates a GitHub Pages site for presentation and tool demo</li>
                </ul>
            </ul>
          </ul>
        </div>      
      </div>

	    <div class="event" id="demo">
        <iframe src="https://aidan2b.shinyapps.io/adr-detection/" width="100%" height="1050px" style="border:none;"></iframe>
      </div>
	  
    </div>
  </div>

  <script src="./scripts.js"></script>
</body>
</html>
