<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Timeline Presentation</title>
  <link rel="stylesheet" href="./styles.css">
</head>
<body>
  <div class="timeline">
    <div class="timeline-navigation">
      <div class="timeline-node active" data-target="event-1">INTRO</div>
      <div class="timeline-node" data-target="event-2">DATA</div>
      <div class="timeline-node" data-target="event-3">ANNOTATION</div>
      <div class="timeline-node" data-target="event-4">TESTING</div>
      <div class="timeline-node" data-target="event-5">PIPELINE</div>
      <div class="timeline-node" data-target="event-6">EVALUATION</div>
      <div class="timeline-node" data-target="event-7">DEPLOY</div>
      <div class="demo-node" data-target="demo">DEMO</div>
    </div>
    <div class="event-container">
      <div class="event" id="event-1"> 
        <div class="event-section">
          <h3 class ="event-title">The Usefulness of Social Media for Understanding Adverse Drug Reactions</h3>
          <p>The rapid expansion and ubiquity of social media platforms have generated vast amounts of user-generated content, offering unprecedented opportunities for the exploration of various research questions. In the realm of pharmacovigilance, investigating the degree to which adverse medication reactions are shared on social media beyond official reporting channels is of particular interest. This study aims to identify potential unacknowledged adverse drug reactions (ADRs) that may be absent from the FDA's Adverse Event Reporting System (FAERS) and determine whether significant outliers exist, while ascertaining the reliability of social media as a data source for pharmacovigilance.</p>
        </div>
        
        <div class="event-section">
          <p>To accomplish this, we will conduct a comparative analysis of the reporting rates of ADRs for the most prescribed medications in the United States, comparing data extracted from social media platforms with that from FAERS. Our measurable objectives include collecting ADR data from both sources, developing ADR extraction methods for social media, comparing social media ADR rates to FAERS baselines, and identifying unrecognized ADRs and significant outliers. Due to constraints, our analysis will be limited to a specific number of medications and will consider Reddit-specific limitations, such as the platform's younger demographic and longer comments. Additionally, we will account for differences in data collection methods and potential social media biases.</p>
        </div>
        
        <div class="event-section">
          <p>Our initial methodology encompasses research methods such as Reddit API utilization and FAERS data mining among a multitude of modelling methods. The intended audience for our findings consists of pharmaceutical, bioinformatics, and public health researchers. We will present our results through a comprehensive website that features visualizations, dashboards, and statistics, offering a detailed exploration of social media data mining for pharmacovigilance.</p>
        </div>
      </div>    
        
      <div class="event" id="event-2"> 
        <div class="event-section">
          <h3 class ="event-title">Data Sources</h3>
          <p>The initial data sources for this study are the FDA's Adverse Event Reporting System (FAERS) and Reddit, as they provide rich and diverse perspectives on adverse drug reactions (ADRs). FAERS is the standard source of information on ADRs reported to the FDA, with voluntary reporting from healthcare providers, patients, and pharmaceutical companies. Reddit, on the other hand, serves as a platform for communities to discuss various diseases and their treatments, offering detailed reports of side effects and potentially unreported ADRs. Utilizing the respective APIs for each data source allows for targeted searches of specific medications.</p>
        </div>
        
        <div class="event-section">
          <p>To efficiently extract Reddit data, we employed the Python Reddit API Wrapper (PRAW) and PushshiftAPI for targeted searches of comments mentioning specific medications. Challenges in Reddit data extraction include the length and grammar of comments, as well as the lack of context, which may hinder accurate interpretation and understanding of the full scope of ADRs. To address these challenges and ensure secure, scalable data storage and processing, we implemented a data management strategy using Google Cloud.</p>
        </div>
        
        <div class="event-section">
          <h3 class ="event-title">Exploratory Data Analysis</h3>
          <p>Sentiment analysis was conducted using the SentimentIntensityAnalyzer on Reddit comment content to determine the polarity scores and assign sentiment labels to DataFrame rows. Visualization of the results was achieved through Seaborn box plots and count plots. Basic topic modeling of negative Reddit comments was performed using Latent Dirichlet Allocation (LDA). Sentiment analysis is a valuable tool for identifying the underlying tone and sentiment of user-generated content, enabling us to uncover potential ADRs that may not be evident through traditional data mining techniques.</p>
          <div class="image-container">
            <img src="visualizations/sent_analysis.png" alt="Sentiment Analysis">
            <img src="visualizations/sent_analysis_dist.png" alt="Sentiment Analysis Distribution">
          </div>
        
        </div>
        

        <div class="event-section">
          <p>To further explore the data, we used the wordcloud library to visualize the most frequently used words per medication and colored these words based on sentiment analysis conducted with the TextBlob library. Moreover, we employed pandas to compare word frequencies across different sentiments, calculating frequency differences for positive and negative Reddit comments. These visualizations and comparisons provide insights into the prominence and distribution of sentiments related to specific medications, allowing for the identification of ADR trends and significant outliers within the data.</p>
          <div class="image-container">
            <img src="visualizations/wordclouds.png" alt="Wordcloud">
            <img src="visualizations/wordfreqcomp.png" alt="Word Frequency Comparison">
          </div>
        </div>
      </div>


	    <div class="event" id="event-3">
        <div class="event-section">
          <h3 class ="event-title"><b>Pivot to Deep Learning</b></h3>
            <p>Recognizing the complexity of our problem, we decided to explore neural networks and deep learning methods under the guidance of our mentor. This required a transformation in our approach, which involved narrowing our scope from multiple medications to a single medication for building a robust model. Subsequently, we aimed to develop a pipeline that could be applied to other drugs.</p>
        </div>
        <div class="event-section">  
          <h3 class ="event-title"><b>spaCy NLP Library Compatibility w/ Prodigy</b></h3>
            <p>To facilitate this transition, we utilized the spaCy NLP library, which is compatible with Prodigy and offers pre-trained models for various annotation styles. In order to train our models on the data, we acknowledged the importance of establishing inter-rater reliability for our annotation method. We annotated raw Reddit text in Prodigy by highlighting symptoms and drugs, saving annotations in JSON format. Furthermore, we incorporated the Medical Dictionary for Regulatory Activities (MedDRA) terminology of side effects to enhance the model's vocabulary.</p>

            <p>Our team divided into pairs to collaboratively annotate a sizable amount of data, ensuring improved model performance. Prodigy allowed multiple users to annotate documents asynchronously, which streamlined the annotation process. This approach also facilitated the collection of annotations from multiple users, guaranteeing high inter-rater reliabilityâ€”a crucial aspect for developing accurate models.</p>
              
            <p>A Google Cloud VM was set up and Prodigy was installed for collaborative annotation work. We redefined our key performance indicators to include Cohen's Kappa calculations, ensuring inter-rater reliability (IRR) between annotator pairs. IRR is essential for maintaining consistency and accuracy in annotation tasks, minimizing subjective bias and variability among annotators, and ultimately enhancing the quality of training data and overall model performance. Cohen's Kappa, a statistical measure that quantifies the level of agreement between two annotators while accounting for chance agreement, served as a valuable metric. Kappa values range from -1 to 1, with 1 indicating perfect agreement and values above 0.6 generally considered acceptable.</p>
        </div>
        <div class="event-section">
          <h3 class ="event-title"><b>Annotation Styles</b></h3>
            <h4>Named Entity Recognition (NER)</h4>
            <p>
              <ul>
                <li>Proper nouns and self-contained expressions like person names or products</li>
                <li>Single-token-based tags; better with clear token boundaries</li>
              </ul>
            </p>
            <img src="visualizations/ner.png" width=550  alt="NER Example" class="padding">
            <h4>Span Categorization (SpanCat)</h4>
            <p>
              <ul>
                <li>Multi-token-based tags; better with complex or overlapping entities</li>
                <li>Can be used to annotate relations between entities</li>
              </ul>
            <img src="visualizations/spancat.png" width=550  alt="SpanCat Example" class="padding">
            <h4>Text Classification (TextCat)</h4>
            <p>
              <ul>
                <li>Binary or multi-class classification of text</li>
                <li>Can be used to annotate text with a single or multiple labels</li>
              </ul>
            <img src="visualizations/textcat.png" width=550  alt="TextCat Example" class="padding">
        </div>   
      </div>

	    <div class="event" id="event-4">
        <div class="event-section">
          <h3 class ="event-title"><b>Testing Models and Annotation Updates</b></h3>
            <h4>Medication Switch</h4>
            <p>
              During our testing phase, we realized that Ocrevus, despite being popular on Reddit, was not an ideal medication for our study due to its relatively low number of adverse drug reactions (ADRs). To provide the model with sufficient ADR data for training, we switched to studying Humiraâ€”a tumor necrosis factor (TNF) blocker known for reducing inflammationâ€”observing that many individuals mentioned and complained about it while annotating Ocrevus comments.
            </p>
            <h4>Model Testing</h4>
            <p>
              In our exploration of Named Entity Recognition (NER) and SpanCat models, we implemented Sense2Vec, a novel method for word sense disambiguation in neural word embeddings that utilizes supervised NLP labels instead of unsupervised clustering. Sense2Vec can disambiguate different parts of speech, sentiment, named entities, and syntactic roles of words, as well as demonstrate subjective and qualitative examples of disambiguated embeddings. This approach provides a more accurate representation of words in context, enhancing our model's ability to identify ADRs.            
            </p>
            <p>
              We also tested a GPT-2 model with a classifier for ADR identification. Our final pipeline's structure emerged from these tests, involving binary classification followed by named entity recognition on classified comments.            
            </p>
            <h4>Annotation Updates</h4>
            <p>
              To expedite the annotation process, we updated our patterns file by incorporating resources such as the ADR Lexicon V 1.1 from HLP Cedars-Sinai, SIDER, CHV, COSTART, and DIEGO_Lab for ADRs, and Drugs@FDA Data Files for drug names and active ingredients. Integrating these resources into our annotation process enabled us to increase annotation speed and improve the quality of our training data, thereby enhancing the overall performance of our model.
            </p>
        </div> 
      </div>

	    <div class="event" id="event-5">
        <div class="event-section">
          <h3 class ="event-title"><b>Pipeline Development</b></h3>
            <h4>Reproducibility</h4>
            <p>
              Our primary goal was to develop a reproducible pipeline that enables the analysis of not only Reddit comments but also extends to other social media platforms, such as Twitter and Facebook. This pipeline aims to be accessible and usable by academic, corporate, governmental, and public entities, allowing users to input any medication or adverse drug reaction of interest.            
            </p>
            <h4>Stage 1: RoBERTa Text Classification</h4>
            <p>
              We employed the RoBERTa text classification model, a robustly optimized BERT pretraining approach built on the classic BERT model with longer and more focused pretraining and hyperparameter optimization. Utilizing the pre-trained RoBERTa base model from the HuggingFace transformers library, we developed custom PyTorch datasets and dataloaders for text preprocessing, which involved removing punctuation and links. The comments were split into lists of strings and passed into the RoBERTa tokenizer item by item with overlap. The PyTorch classification head on top of the RoBERTa base model takes the pooled output and performs classification (ADR or no ADR) with dropout layers to prevent overfitting. We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler.            
            </p>
            <p>
              To augment text classification training data, we employed OpenAI's GPT-3.5-turbo model for generating and classifying comments, addressing class imbalances and diversifying the training data to counteract biases and improve annotation efficiency.            
            </p>
            <h4>Stage 2: Flair NER</h4>
            <p>
              For Named Entity Recognition (NER), we used Flair's SequenceTagger with stacked embeddings, including GloVe and Flair embeddings, providing different embeddings for the same word depending on its contextual use. We incorporated word dropout and locked dropout to prevent overfitting, as well as a bidirectional Long Short-Term Memory (biLSTM) RNN to maintain short-term memory throughout the input sequence processing. We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler, employing the ViterbiLoss function.            
            </p>
            <h4>Stage 3: spaCy Dependency Parser</h4>
            <p>
              Finally, we utilized a pre-trained spaCy dependency parser to extract and pair drug-ADR entities based on the output of the SequenceTagger. This parser links drugs and ADRs in the input text by finding the shortest dependency path between them, using the en_core_web_md model. The extracted and paired drug-ADR entities were saved in a CSV file for further analysis.            </p>
            <h4>Pipeline Summary</h4>
            <p>
              This pipeline offers a robust and flexible approach to extracting ADR phrases in relation to medications in text, making it an optimal method for our project's objectives. By combining advanced text classification, NER, and dependency parsing techniques, our pipeline can efficiently identify and extract relevant information from social media data for pharmacovigilance purposes.            </p>
        </div> 
      </div>

      <div class="event" id="event-6">
        <div class="event-section">
          <h3 class ="event-title"><b>Pipeline Evaluation and Optimization</b></h3>
            <h4>Finalizing Annotation Methods</h4>
            <p>
              During the final annotation phase, Taylor and Zach maintained a satisfactory Cohen's Kappa for text classification, while Jackson and Aidan initially achieved poor results. To address this issue, the team created a document outlining rules for both annotation methods. Subsequently, Jackson and Aidan significantly improved their Cohen's Kappa scores, achieving better consistency and accuracy in their annotations. The inter-rater reliability improvements were as follows:

              <ul><li>Text Classification (from previous TextCat):</li>
                    <ul>
                      <li>Taylor and Zach: 0.74 -> 0.73</li>
                    </ul>
                  <li>Named Entity Recognition:</li>
                  <ul>
                    <li>Aidan and Jackson: 0.38 -> 0.58</li>
                  </ul>
              </ul>            
            </p>
            <h4>RoBERTa Text Classification</h4>
            <p>
              To evaluate the RoBERTa model, we split the data into training, validation, and test sets. The model was trained using a custom PyTorch data loader and the RoBERTa tokenizer, which is a byte pair encoding (BPE) tokenizer. We ran the model for five epochs with a validation cycle after each epoch and calculated the training and validation accuracy and loss metrics at each step to observe convergence.            
            </p>
            <img src="visualizations/robertatrainviz.png" alt="RoBERTa Training" class="padding">
            <p>
              To augment text classification training data, we employed OpenAI's GPT-3.5-turbo model for generating and classifying comments, addressing class imbalances and diversifying the training data to counteract biases and improve annotation efficiency.            
            </p>
            <h4>Flair NER</h4>
            <p>
              For Named Entity Recognition (NER), we used Flair's SequenceTagger with stacked embeddings, including GloVe and Flair embeddings, providing different embeddings for the same word depending on its contextual use. We incorporated word dropout and locked dropout to prevent overfitting, as well as a bidirectional Long Short-Term Memory (biLSTM) RNN to maintain short-term memory throughout the input sequence processing. We trained the model using 5 epochs with validation cycles, CrossEntropyLoss with class weights, the AdamW optimizer, and a linear learning rate scheduler, employing the ViterbiLoss function.            
            </p>
            <div class="image-container">
              <img src="visualizations/flairjustglove.png" alt="NER Visualization">
              <img src="visualizations/flairembedplusglove.png" alt="NER Visualization">
            </div>
            <img src="visualizations/flairtrainviz.png" alt="Flair Training" class="padding">
            <h4>spaCy Dependency Parser</h4>
            <p>
              For our dependency parser implementation, we have developed an interactive network graph that visually represents the relationships between drug and ADR entities extracted using the NER model. This graph, constructed by generating nodes for each unique drug and ADR entity and forming edges based on the dependency relations identified by the spaCy dependency parser, allows users to explore the connections between drugs and their associated ADRs, thereby providing an intuitive way to assess the quality of the entity extraction and dependency parsing processes. The interactive visualization enables users to click on individual nodes and view the corresponding text containing the extracted pairing, offering valuable insights into the context in which the entities were identified and the accuracy of the dependency parsing process.
            </p>
            <img src="visualizations/dependency-viz.png" width="550" alt="Dependency Parser Visualization" class="padding">
            <p>
              This evaluation method offers several advantages for understanding the performance of the NER and dependency parsing models, such as facilitating a more intuitive understanding of the underlying structure of the extracted entities and their associations, and allowing users to explore specific instances of drug-ADR pairings to assess their validity based on the surrounding context. Furthermore, the integration of NER results and dependency parser output enables users to evaluate the overall effectiveness of the pipeline in identifying and connecting relevant entities.            
            </p>
            <h4>Evaluation Summary</h4>
            <p>
              Discuss model results comprehensively.
            </p>
        </div> 
      </div>
      <div class="event" id="event-7">
        <div class="event-section"> 
          <h3 class ="event-title"><b>Docker Containers for Data Storage and Reproducibility</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage with two initial versions (v1: 1200 comments, v1-large: 12000 comments)</li>
            <li>Ensures reproducibility through containerized environments</li>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>GitHub Repository and Actions Workflows</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage and version control</li>
            <li>GitHub-hosted CPU runners and self-hosted GPU runners</li>
            <li>Initial workflows:</li>
            <ul>
                <li>Push select GitHub branch to a specific Docker tag</li>
                <li>Enter medication, select Docker tag, run pipeline, and deploy to Shiny Apps (on self-hosted runner)</li>
            </ul>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>Challenges and Workflow Adjustments</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Aidan's laptop's RTX 2060 Max-Q can only do so much...</li>
            <li>Running pipeline for each user request is time-consuming and impractical</li>
            <li>New strategy considered for more efficient processing</li>
            <li>PushShift API issues encountered</li>
            <li>Config file refresh leads to hiccups when running, usually needs to be reset once daily, but works otherwise.</li>
          </ul>
        </div>
        <div class="event-section"> 
          <h3 class ="event-title"><b>Nautilus HyperCluster</b></h3>
          <ul style="line-height:1.5" class ="event-content"> 
            <li>Remote storage, CPU and GPU runners</li>
            <li>Initial workflows:</li>
            <ul>
                <li>Run pipeline and deploy to Shiny Apps</li>
            </ul>
            <li>Current Workflow:</li>
            <ul>
                <li>Create a database of Reddit comments using Medicrawl.sh</li>
                <ul>
                    <li>Pulls 1000 comments for each medication listed in ConfigMap file</li>
                    <li>Appends comments to CSVs stored in Persistent Volume Claim (PVC)</li>
                </ul>
                <li>GitHub and Nautilus integration:</li>
                <ul>
                    <li>Push to Shiny App: loads CSVs from PVC and builds and deploys Shiny App</li>
                    <li>Push to Pages: creates a GitHub Pages site for presentation and tool demo</li>
                </ul>
            </ul>
          </ul>
        </div>      
      </div>

	    <div class="event" id="demo">
        <iframe src="https://aidan2b.shinyapps.io/adr-detection/" width="100%" height="1050px" style="border:none;"></iframe>
      </div>
	  
    </div>
  </div>

  <script src="./scripts.js"></script>
</body>
</html>
